{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59b77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import re\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "random.seed(122090791)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7eb86c",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/partA/eng-cmn.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "pairs = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) != 3:\n",
    "        # print(f\"跳过无效行: {line}\")\n",
    "        continue\n",
    "    en, zh, _ = parts\n",
    "    \n",
    "    # clean english text\n",
    "    en_clean = re.sub(r'[^\\w\\s]', '', en.strip().lower()).strip()\n",
    "    # clean chinese text\n",
    "    zh_clean = re.sub(r'[^\\w\\s]', '', zh.strip()).strip()\n",
    "    \n",
    "    # skip empty lines\n",
    "    if not en_clean or not zh_clean:\n",
    "        # print(f\"跳过空文本行: {line}\")\n",
    "        continue\n",
    "    \n",
    "    pairs.append((zh_clean, en_clean))  # chi -> eng\n",
    "\n",
    "# set and filter pairs\n",
    "seen = set()\n",
    "cleaned_pairs = []\n",
    "for pair in pairs:\n",
    "    if pair not in seen:\n",
    "        seen.add(pair)\n",
    "        cleaned_pairs.append(pair)\n",
    "\n",
    "max_length = 128\n",
    "filtered_pairs = [\n",
    "    (zh, en) for zh, en in cleaned_pairs\n",
    "    if len(zh) <= max_length and len(en) <= max_length\n",
    "]\n",
    "\n",
    "# split dataset\n",
    "random.shuffle(filtered_pairs)\n",
    "total = len(filtered_pairs)\n",
    "train_pairs = filtered_pairs[:int(0.9*total)]\n",
    "test_pairs = filtered_pairs[int(0.9*total):]\n",
    "\n",
    "# train tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "def corpus_iterator():\n",
    "    for zh, en in train_pairs:\n",
    "        yield zh  # chi input\n",
    "        yield en  # eng target\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=corpus_iterator(),\n",
    "    vocab_size=10000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[SOS]\", \"[EOS]\", \"[PAD]\", \"[UNK]\", \"[SEP]\"]\n",
    ")\n",
    "\n",
    "# generate token ids\n",
    "SOS_ID = tokenizer.token_to_id(\"[SOS]\")\n",
    "SEP_ID = tokenizer.token_to_id(\"[SEP]\")\n",
    "EOS_ID = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "def encode_pair(zh, en):\n",
    "    # input: [SOS] chi [SEP] eng [EOS]\n",
    "    input_text = f\"[SOS]{zh}[SEP]{en}[EOS]\"\n",
    "    return tokenizer.encode(input_text).ids\n",
    "\n",
    "def process_encodings(encodings):\n",
    "    input_ids_list, target_ids_list, loss_mask_list = [], [], []\n",
    "    for encoding in encodings:\n",
    "        input_ids = encoding[:-1]\n",
    "        target_ids = encoding[1:]\n",
    "        sep_pos = encoding.index(SEP_ID) + 1\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        for i in range(sep_pos, len(input_ids)):\n",
    "            loss_mask[i] = 1\n",
    "        input_ids_list.append(input_ids)\n",
    "        target_ids_list.append(target_ids)\n",
    "        loss_mask_list.append(loss_mask)\n",
    "    return input_ids_list, target_ids_list, loss_mask_list\n",
    "\n",
    "train_encodings = [encode_pair(zh, en) for zh, en in train_pairs]\n",
    "test_encodings = [encode_pair(zh, en) for zh, en in test_pairs]\n",
    "\n",
    "train_inputs, train_targets, train_masks = process_encodings(train_encodings)\n",
    "test_inputs, test_targets, test_masks = process_encodings(test_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e59cb3",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161732e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数量: 11.7M\n"
     ]
    }
   ],
   "source": [
    "class GPTTranslator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # word embedding layer\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(1024, d_model)  # position embedding layer\n",
    "        \n",
    "        # Transformer Decoder层\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # output layer\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, memory=None, tgt_mask=None):\n",
    "        device = x.device\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # position encoding\n",
    "        positions = torch.arange(0, seq_len, device=device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "        \n",
    "        # word embedding + position encoding\n",
    "        tok_emb = self.token_emb(x)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # generate mask\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(device)\n",
    "        \n",
    "        memory = torch.zeros((batch_size, 1, self.d_model), device=device)\n",
    "\n",
    "        # Transformer forward pass\n",
    "        out = self.transformer(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "        )\n",
    "        \n",
    "        # predict logits\n",
    "        logits = self.lm_head(out)\n",
    "        return logits\n",
    "\n",
    "# get vocab size\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "\n",
    "model = GPTTranslator(vocab_size=vocab_size)\n",
    "# print(f\"参数量: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c99e5",
   "metadata": {},
   "source": [
    "## loss fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(logits, targets, mask):\n",
    "    # flat\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flat = targets.view(-1)\n",
    "    mask_flat = mask.view(-1).bool()\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "    masked_loss = loss * mask_flat\n",
    "    \n",
    "    # average\n",
    "    return masked_loss.sum() / mask_flat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8341666",
   "metadata": {},
   "source": [
    "## batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data_inputs, data_targets, data_masks, batch_size=32):\n",
    "    indices = list(range(len(data_inputs)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        \n",
    "        # find the max length of the current batch\n",
    "        max_len = max(len(data_inputs[idx]) for idx in batch_indices)\n",
    "        \n",
    "        # fill the current batch with padding to the max length\n",
    "        batch_inputs = []\n",
    "        batch_targets = []\n",
    "        batch_masks = []\n",
    "        for idx in batch_indices:\n",
    "            inputs = data_inputs[idx]\n",
    "            targets = data_targets[idx]\n",
    "            mask = data_masks[idx]\n",
    "            \n",
    "            # fill the current sequence with padding to the max length\n",
    "            pad_len = max_len - len(inputs)\n",
    "            pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "            \n",
    "            batch_inputs.append(inputs + [pad_id] * pad_len)\n",
    "            batch_targets.append(targets + [pad_id] * pad_len)\n",
    "            batch_masks.append(mask + [0] * pad_len)\n",
    "        \n",
    "        yield (\n",
    "            torch.tensor(batch_inputs),\n",
    "            torch.tensor(batch_targets),\n",
    "            torch.tensor(batch_masks).float()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa08bc",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43463a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig('plot_gpt.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5956ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\86183\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 28s (- 36m 18s) (5 3%) 0.1471\n",
      "3m 1s (- 35m 42s) (10 7%) 0.0989\n",
      "4m 37s (- 34m 47s) (15 11%) 0.0658\n",
      "6m 13s (- 33m 36s) (20 15%) 0.0413\n",
      "7m 53s (- 32m 30s) (25 19%) 0.0248\n",
      "9m 27s (- 30m 55s) (30 23%) 0.0150\n",
      "11m 3s (- 29m 22s) (35 27%) 0.0100\n",
      "12m 42s (- 27m 57s) (40 31%) 0.0077\n",
      "14m 16s (- 26m 20s) (45 35%) 0.0063\n",
      "15m 49s (- 24m 40s) (50 39%) 0.0056\n",
      "17m 27s (- 23m 9s) (55 42%) 0.0050\n",
      "19m 2s (- 21m 35s) (60 46%) 0.0046\n",
      "20m 36s (- 19m 58s) (65 50%) 0.0042\n",
      "22m 8s (- 18m 21s) (70 54%) 0.0040\n",
      "23m 41s (- 16m 44s) (75 58%) 0.0038\n",
      "25m 21s (- 15m 13s) (80 62%) 0.0036\n",
      "26m 57s (- 13m 38s) (85 66%) 0.0035\n",
      "28m 35s (- 12m 4s) (90 70%) 0.0033\n",
      "30m 9s (- 10m 28s) (95 74%) 0.0032\n",
      "31m 47s (- 8m 54s) (100 78%) 0.0031\n",
      "33m 20s (- 7m 18s) (105 82%) 0.0030\n",
      "34m 53s (- 5m 42s) (110 85%) 0.0030\n",
      "36m 23s (- 4m 6s) (115 89%) 0.0029\n",
      "37m 54s (- 2m 31s) (120 93%) 0.0028\n",
      "39m 22s (- 0m 56s) (125 97%) 0.0027\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 128\n",
    "print_every = 5\n",
    "plot_every = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    batch_iter = batch_generator(train_inputs, train_targets, train_masks, batch_size=32)\n",
    "    \n",
    "    for batch_inputs, batch_targets, batch_masks in batch_iter:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        batch_masks = batch_masks.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        logits = model(batch_inputs)\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = masked_loss(logits, batch_targets, batch_masks)\n",
    "        \n",
    "        # backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print_loss_total += total_loss/len(train_inputs)\n",
    "    plot_loss_total += total_loss/len(train_inputs)\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "    \n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "    \n",
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337852c",
   "metadata": {},
   "source": [
    "## translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(zh_sentence, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # encoder_input_ids: [SOS] chi [SEP]\n",
    "        input_text = f\"[SOS]{zh_sentence}[SEP]\"\n",
    "        input_ids = tokenizer.encode(input_text).ids\n",
    "        input_tensor = torch.tensor([input_ids]).to(device)\n",
    "        \n",
    "        # generate\n",
    "        for _ in range(max_len):\n",
    "            logits = model(input_tensor)\n",
    "            next_token = logits[0, -1].argmax()\n",
    "            if next_token == EOS_ID:\n",
    "                break\n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # decode\n",
    "        output_ids = input_tensor[0].tolist()\n",
    "        sep_pos = output_ids.index(SEP_ID)\n",
    "        en_ids = output_ids[sep_pos+1:]\n",
    "        return tokenizer.decode(en_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5686e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 湯姆幫瑪麗買了她所有需要買的東西\n",
      "= tom helped mary buy everything she needed\n",
      "< <EOS>\n",
      "\n",
      "> 老人們很早就起床\n",
      "= old people get up very early\n",
      "< privacy is good for eight hours <EOS>\n",
      "\n",
      "> 他给我讲述了他的一生\n",
      "= he told me the story of his life\n",
      "< his mistakes to me his took took took <EOS>\n",
      "\n",
      "> 汤姆总是旷课\n",
      "= tom is always absent\n",
      "< is always fighting <EOS>\n",
      "\n",
      "> 你能冷凍它嗎\n",
      "= can you freeze it\n",
      "< you cold in it <EOS>\n",
      "\n",
      "> 如果你不介意的话我想一个人呆着\n",
      "= id like to be alone if you dont mind\n",
      "< not like to go out how long time <EOS>\n",
      "\n",
      "> 你現在有空嗎\n",
      "= are you free right now\n",
      "< free free is free <EOS>\n",
      "\n",
      "> 我燒了紙\n",
      "= i burned the paper\n",
      "< got a paper <EOS>\n",
      "\n",
      "> 我現在正在彈鋼琴\n",
      "= i am playing the piano now\n",
      "< is playing the piano playing the piano <EOS>\n",
      "\n",
      "> 为什么你不来看我们\n",
      "= why dont you come visit us\n",
      "< will not have enough for us <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_translation(test_pairs, num_samples=10):\n",
    "    samples = random.sample(test_pairs, min(num_samples, len(test_pairs)))\n",
    "    \n",
    "    for zh, en_true in samples:\n",
    "        en_pred = translate(zh)\n",
    "        en_pred += \" <EOS>\"\n",
    "        \n",
    "        print(f\"> {zh}\")\n",
    "        print(f\"= {en_true}\")\n",
    "        print(f\"<{en_pred}\\n\")\n",
    "\n",
    "evaluate_translation(test_pairs, num_samples=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
