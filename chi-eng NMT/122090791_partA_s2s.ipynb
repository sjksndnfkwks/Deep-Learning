{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train BPETokenizer \n",
    "def train_bpe_tokenizer(texts, vocab_size=26500, min_frequency=2, special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\"]):\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# construct word2index, word2count and index2word dicts\n",
    "def build_dicts(tokenizer, texts):\n",
    "    word2index = tokenizer.get_vocab()\n",
    "    word2count = {token: 0 for token in word2index.keys()}\n",
    "    index2word = {index: token for token, index in word2index.items()}\n",
    "\n",
    "    for text in texts:\n",
    "        output = tokenizer.encode(text)\n",
    "        tokens = output.tokens\n",
    "        for token in tokens:\n",
    "            word2count[token] += 1\n",
    "\n",
    "    return word2index, word2count, index2word\n",
    "\n",
    "def normalizeStringEng(s):\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    s = s.lower().strip()\n",
    "    return s\n",
    "\n",
    "def normalizeStringChi(s):\n",
    "    s = re.sub(r'[^\\u4e00-\\u9fa5\\s]', '', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "# read corpus\n",
    "def read_corpus(file_path):\n",
    "    english_texts = []\n",
    "    chinese_texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                english_texts.append(normalizeStringEng(parts[0]))\n",
    "                chinese_texts.append(normalizeStringChi(parts[1]))\n",
    "    return english_texts, chinese_texts\n",
    "\n",
    "\n",
    "file_path = \"data/raw/partA/eng-cmn.txt\"  \n",
    "english_texts, chinese_texts = read_corpus(file_path)\n",
    "\n",
    "pairs = list(zip(english_texts, chinese_texts))\n",
    "# train english tokenizer\n",
    "english_tokenizer = train_bpe_tokenizer(english_texts)\n",
    "english_word2index, english_word2count, english_index2word = build_dicts(english_tokenizer, english_texts)\n",
    "\n",
    "# train chinese tokenizer\n",
    "chinese_tokenizer = train_bpe_tokenizer(chinese_texts)\n",
    "chinese_word2index, chinese_word2count, chinese_index2word = build_dicts(chinese_tokenizer, chinese_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(english_texts[:5])\n",
    "# print(chinese_texts[:5])\n",
    "# print(pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出部分结果示例\n",
    "# print(english_word2index)\n",
    "# print(len(english_word2index))\n",
    "# print(english_index2word[2])\n",
    "# print(len(english_index2word))\n",
    "# sorted_index2word = {key: english_index2word[key] for key in sorted(english_index2word.keys())}\n",
    "# print(sorted_index2word)\n",
    "\n",
    "\n",
    "# print(english_word2count)\n",
    "\n",
    "# print(chinese_word2index)\n",
    "# print(len(chinese_word2index))\n",
    "# print(chinese_index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "pairs = filterPairs(pairs)\n",
    "pairs = [list(reversed(p)) for p in pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "\n",
    "def indexesFromSentence(tokenizer, sentence):\n",
    "    # print(sentence)\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    return encoded.ids\n",
    "\n",
    "def index2sentence(indexes, tokenizer, skip_special_tokens=True):\n",
    "\n",
    "    tokens = []\n",
    "    for idx in indexes:\n",
    "        token = tokenizer.id_to_token(idx)\n",
    "        \n",
    "        # handle special tokens\n",
    "        if skip_special_tokens and token in ['[UNK]', '[SOS]', '[EOS]']:\n",
    "            continue\n",
    "        \n",
    "        # handle subword tokens\n",
    "        if token.startswith('##'):\n",
    "            tokens[-1] += token[2:]\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    \n",
    "    return ''.join(tokens)\n",
    "\n",
    "def tensorFromSentence(tokenizer, sentence):\n",
    "    indexes = indexesFromSentence(tokenizer, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size):\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        \n",
    "        inp_ids = indexesFromSentence(chinese_tokenizer, inp )[:MAX_LENGTH-1]\n",
    "        tgt_ids = indexesFromSentence(english_tokenizer, tgt)[:MAX_LENGTH-1]\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        # print(f\"inp_ids: {inp_ids}\")\n",
    "        # print(f\"tgt_ids: {tgt_ids}\")\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    dataset = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                            torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_size = int(0.9 * n)\n",
    "\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "\n",
    "    train_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_data = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig('plot_s2s.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 55s (- 22m 49s) (5 3%) 2.0359\n",
      "1m 51s (- 21m 50s) (10 7%) 0.4135\n",
      "2m 46s (- 20m 52s) (15 11%) 0.1495\n",
      "3m 41s (- 19m 53s) (20 15%) 0.1276\n",
      "4m 33s (- 18m 45s) (25 19%) 0.1176\n",
      "5m 25s (- 17m 43s) (30 23%) 0.1101\n",
      "6m 17s (- 16m 43s) (35 27%) 0.1068\n",
      "7m 8s (- 15m 43s) (40 31%) 0.1034\n",
      "8m 3s (- 14m 52s) (45 35%) 0.1016\n",
      "8m 55s (- 13m 54s) (50 39%) 0.1009\n",
      "9m 46s (- 12m 58s) (55 42%) 0.0998\n",
      "10m 37s (- 12m 2s) (60 46%) 0.0985\n",
      "11m 30s (- 11m 9s) (65 50%) 0.0977\n",
      "12m 24s (- 10m 16s) (70 54%) 0.0970\n",
      "13m 13s (- 9m 20s) (75 58%) 0.0969\n",
      "14m 5s (- 8m 27s) (80 62%) 0.0966\n",
      "14m 51s (- 7m 30s) (85 66%) 0.0962\n",
      "15m 37s (- 6m 35s) (90 70%) 0.0964\n",
      "16m 26s (- 5m 42s) (95 74%) 0.0960\n",
      "17m 14s (- 4m 49s) (100 78%) 0.0963\n",
      "18m 3s (- 3m 57s) (105 82%) 0.0976\n",
      "18m 52s (- 3m 5s) (110 85%) 0.0978\n",
      "19m 41s (- 2m 13s) (115 89%) 0.0956\n",
      "20m 30s (- 1m 22s) (120 93%) 0.0972\n",
      "21m 18s (- 0m 30s) (125 97%) 0.0963\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(len(chinese_word2count), hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(english_word2count)).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 128, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(chinese_tokenizer, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(english_index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 我曾與他的比賽網球\n",
      "= i had a tennis match with him\n",
      "< i felt a tennis match with him <EOS>\n",
      "\n",
      "> 我举办了一场派对\n",
      "= i threw a party\n",
      "< i grow on me <EOS>\n",
      "\n",
      "> 欢迎来到我们家\n",
      "= welcome to our home\n",
      "< welcome to our home <EOS>\n",
      "\n",
      "> 我没有反对你\n",
      "= im not disagreeing with you\n",
      "< im not disagree ing with you <EOS>\n",
      "\n",
      "> 該建築高一百公尺\n",
      "= the building is one hundred meters high\n",
      "< the day passes one s were ready <EOS>\n",
      "\n",
      "> 我看见一个穿黑衣服的女人\n",
      "= i saw a woman in black\n",
      "< come down <EOS>\n",
      "\n",
      "> 每个人都知道汤姆的法语很好\n",
      "= everyone knows that toms french is good\n",
      "< everyone knew tom was good at french <EOS>\n",
      "\n",
      "> 你這個星期一直在做什麼\n",
      "= what have you been doing this week\n",
      "< what have you been doing this week <EOS>\n",
      "\n",
      "> 為甚麼他不再跟我講笑話了\n",
      "= why doesnt he tell me jokes anymore\n",
      "< why he was wasting you and drinking <EOS>\n",
      "\n",
      "> 突然開始下雨\n",
      "= all of a sudden it began raining\n",
      "< it suddenly started raining <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 嗨。\n",
      "output = hi where does your uncle name <EOS>\n",
      "input = 你用跑的。\n",
      "output = run <EOS>\n",
      "input = 我已经起来了。\n",
      "output = ive already watch my time <EOS>\n",
      "input = 静静的，别动。\n",
      "output = please leave me alone <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 21992 (\\N{CJK UNIFIED IDEOGRAPH-55E8}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 20320 (\\N{CJK UNIFIED IDEOGRAPH-4F60}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 29992 (\\N{CJK UNIFIED IDEOGRAPH-7528}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 36305 (\\N{CJK UNIFIED IDEOGRAPH-8DD1}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 25105 (\\N{CJK UNIFIED IDEOGRAPH-6211}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 24050 (\\N{CJK UNIFIED IDEOGRAPH-5DF2}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 32463 (\\N{CJK UNIFIED IDEOGRAPH-7ECF}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 36215 (\\N{CJK UNIFIED IDEOGRAPH-8D77}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 20102 (\\N{CJK UNIFIED IDEOGRAPH-4E86}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:8: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 38745 (\\N{CJK UNIFIED IDEOGRAPH-9759}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 65292 (\\N{FULLWIDTH COMMA}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 21035 (\\N{CJK UNIFIED IDEOGRAPH-522B}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 21160 (\\N{CJK UNIFIED IDEOGRAPH-52A8}) missing from current font.\n",
      "  fig.savefig(filename)\n",
      "C:\\Users\\86183\\AppData\\Local\\Temp\\ipykernel_25312\\902765783.py:17: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from current font.\n",
      "  fig.savefig(filename)\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions, filename):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # plt.show()\n",
    "    fig.savefig(filename)\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence, filename):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :], filename)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('嗨。', \"嗨attnMap.png\")\n",
    "\n",
    "evaluateAndShowAttention('你用跑的。', \"你用跑的attnMap.png\")\n",
    "\n",
    "evaluateAndShowAttention('我已经起来了。', \"我已经起来了attnMap.png\")\n",
    "\n",
    "evaluateAndShowAttention('静静的，别动。', \"静静的别动attnMap.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
