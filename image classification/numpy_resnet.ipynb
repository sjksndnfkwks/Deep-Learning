{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Code Example - Part B\n",
    "\n",
    "This achieves an accuracy of 93.26% on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide some basic operators like matrix multiplication\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.params = []  # 实例变量，用于存储参数\n",
    "\n",
    "# sequential module    \n",
    "class Sequential(Module):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for layer in layers:\n",
    "            self.params.extend(layer.params)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus\n",
    "\n",
    "This implements the [Softplus](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html) function.\n",
    "\n",
    "$y = \\frac{1}{\\beta} \\ln(1+e^{\\beta x})$\n",
    "\n",
    "$y' = \\frac{1}{1+e^{-\\beta x}}$\n",
    "\n",
    "Default: $\\beta=1$\n",
    "\n",
    "$e^{\\beta x}$ might be too large and unstable; so we use linear function to approximate it when $\\beta x$ is above the threshold $20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softplus(Module):\n",
    "    def __init__(self, beta=1.0, threshold=20.0):\n",
    "        assert beta > 0 and threshold > 0\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.beta_x = self.beta * x # save the input for backward use\n",
    "        y = np.log(1 + np.exp(self.beta_x)) / self.beta\n",
    "        y_relu = np.where(x > 0, x, 0)\n",
    "        return np.where(x < self.threshold, y, y_relu)\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        grad = 1 / (1 + np.exp(-self.beta_x))\n",
    "        grad_relu = np.where(self.beta_x > 0, 1, 0)\n",
    "        return dy * np.where(self.beta_x < self.threshold, grad, grad_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearNoBias\n",
    "\n",
    "This implements the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer but without the bias term.\n",
    "\n",
    "$y = x W^T$\n",
    "\n",
    "$dy/dx = W$\n",
    "\n",
    "$dy/dW = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoBias(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weight = (np.random.rand(out_features, in_features) * 2 - 1) / in_features ** 0.5\n",
    "        self.weight_grad = np.zeros_like(self.weight)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [dict(val=self.weight, grad=self.weight_grad)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weight.T\n",
    "\n",
    "    def backward(self, dy):\n",
    "        self.weight_grad[:] = dy.T @ self.x\n",
    "        return dy @ self.weight\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n",
    "\n",
    "This implements the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(x, num_classes=10):\n",
    "    y = np.zeros([len(x), num_classes])\n",
    "    y[np.arange(len(x)), x] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Module):    \n",
    "    def forward(self, x_logit, x_target):\n",
    "        self.x_logit = x_logit\n",
    "        self.x_target = x_target\n",
    "        \n",
    "        # softmax\n",
    "        x_logit_sub = np.exp(x_logit - np.max(x_logit, axis=1, keepdims=True))\n",
    "        x_softmax = x_logit_sub / np.sum(x_logit_sub, axis=1, keepdims=True)\n",
    "        x_softmax = np.clip(x_softmax, a_min=1e-15, a_max=None) # to avoid zero values\n",
    "        self.x_softmax = x_softmax # save for backward\n",
    "\n",
    "        # loss of each item\n",
    "        loss_x = -np.log(x_softmax)[np.arange(len(x_target)), x_target]\n",
    "\n",
    "        # average\n",
    "        return loss_x.mean()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * (self.x_softmax - onehot(self.x_target)) / len(self.x_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optim:\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p[\"grad\"][...] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optim):\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        super().__init__(params, lr) \n",
    "        self.momentum = momentum\n",
    "        self.velocities = [np.zeros_like(p[\"val\"]) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            # v = momentum * v + grad\n",
    "            self.velocities[i] = self.momentum * self.velocities[i] + p[\"grad\"]\n",
    "            # param = param - lr * v\n",
    "            p[\"val\"] -= self.lr * self.velocities[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, negative_slope=0.01):\n",
    "    return np.where(x > 0, x, x * negative_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # initialize weight and bias\n",
    "        scale = np.sqrt(2.0 / in_features)\n",
    "        self.weight = {'val': np.random.randn(in_features, out_features) * scale,\n",
    "                       'grad': np.zeros((in_features, out_features))}\n",
    "        self.params.append(self.weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = {'val': np.zeros(out_features),\n",
    "                         'grad': np.zeros(out_features)}\n",
    "            self.params.append(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        # initialize cache to store input for forward pass\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # store input for backward pass\n",
    "        self.cache = x\n",
    "        out = x @ self.weight['val']\n",
    "        if self.bias is not None:\n",
    "            out += self.bias['val']\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # compute gradients\n",
    "        x = self.cache\n",
    "        self.weight['grad'] += x.T @ dout\n",
    "        if self.bias is not None:\n",
    "            self.bias['grad'] += dout.sum(axis=0)\n",
    "        return dout @ self.weight['val'].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride if isinstance(stride, tuple) else (stride, stride)\n",
    "        self.bias = bias\n",
    "\n",
    "        # he initialization scale factor\n",
    "        scale = np.sqrt(2.0 / (in_channels * np.prod(self.kernel_size)))\n",
    "        # initialize weight with random values and zero gradients\n",
    "        self.weight = {\n",
    "            'val': np.random.randn(out_channels, in_channels, *self.kernel_size) * scale,\n",
    "            'grad': np.zeros((out_channels, in_channels, *self.kernel_size))\n",
    "        }\n",
    "        self.params.append(self.weight)\n",
    "\n",
    "        if self.bias:\n",
    "            self.bias_param = {'val': np.zeros(out_channels),\n",
    "                               'grad': np.zeros(out_channels)}\n",
    "            self.params.append(self.bias_param)\n",
    "        else:\n",
    "            self.bias_param = None\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get input dimension\n",
    "        N, C, H, W = x.shape\n",
    "        # kernel height and width\n",
    "        K_h, K_w = self.kernel_size\n",
    "        # stride height and width\n",
    "        S_h, S_w = self.stride\n",
    "        # padding height and width\n",
    "        P_h, P_w = K_h // 2, K_w // 2\n",
    "\n",
    "        # calculate output dimension\n",
    "        H_out = (H + 2 * P_h - K_h) // S_h + 1\n",
    "        W_out = (W + 2 * P_w - K_w) // S_w + 1\n",
    "\n",
    "        # pad input with zeros\n",
    "        x_pad = np.pad(x, [(0, 0), (0, 0), (P_h, P_h), (P_w, P_w)], mode='constant')\n",
    "        \n",
    "        \n",
    "        # im2col\n",
    "        cols = np.zeros((N, C, K_h, K_w, H_out, W_out))\n",
    "\n",
    "        for i in range(K_h):\n",
    "            for j in range(K_w):\n",
    "                cols[:, :, i, j, :, :] = x_pad[:, :, i:i + S_h * H_out:S_h, j:j + S_w * W_out:S_w]\n",
    "\n",
    "        cols = cols.transpose(0, 4, 5, 1, 2, 3).reshape(N * H_out * W_out, -1)\n",
    "        w_col = self.weight['val'].reshape(self.out_channels, -1).T\n",
    "\n",
    "        # print(f\"cols shape: {cols.shape}\")\n",
    "        # print(f\"w_col shape: {w_col.shape}\")\n",
    "\n",
    "        # convolution\n",
    "        out = cols @ w_col\n",
    "        if self.bias_param is not None:\n",
    "            out += self.bias_param['val']\n",
    "\n",
    "        out = out.reshape(N, H_out, W_out, self.out_channels).transpose(0, 3, 1, 2)\n",
    "        # cache values needed for backward pass\n",
    "        self.cache = (x, cols, w_col.shape, (H_out, W_out), (P_h, P_w), S_h, S_w)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # retrieve cached values\n",
    "        x, cols, w_shape, (H_out, W_out), (P_h, P_w), S_h, S_w = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        K_h, K_w = self.kernel_size\n",
    "\n",
    "        # flatten output gradient\n",
    "        dout_flat = dout.transpose(0, 2, 3, 1).reshape(-1, self.out_channels)\n",
    "        # print(f\"dout_flat shape: {dout_flat.shape}\")\n",
    "        \n",
    "        # calculate weight gradient\n",
    "        dw = cols.T @ dout_flat\n",
    "        self.weight['grad'] += dw.T.reshape(self.weight['val'].shape)\n",
    "\n",
    "        if self.bias_param is not None:\n",
    "            self.bias_param['grad'] += dout_flat.sum(axis=0)\n",
    "\n",
    "        w_flat = self.weight['val'].reshape(self.out_channels, -1)\n",
    "        # print(f\"w_flat.T shape: {w_flat.T.shape}\")\n",
    "\n",
    "        # check whether dimension matches\n",
    "        if dout_flat.shape[1] != w_flat.shape[0]:\n",
    "            raise ValueError(f\"Dimension mismatch: dout_flat shape {dout_flat.shape}, w_flat shape {w_flat.shape}\")\n",
    "\n",
    "        dcols = dout_flat @ w_flat\n",
    "        dcols = dcols.reshape(N, H_out, W_out, C, K_h, K_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "        # padded input gradient\n",
    "        dx_pad = np.zeros((N, C, H + 2 * P_h, W + 2 * P_w))\n",
    "        for i in range(K_h):\n",
    "            for j in range(K_w):\n",
    "                dx_pad[:, :, i:i + H_out * S_h:S_h, j:j + W_out * S_w:S_w] += dcols[:, :, i, j, :, :]\n",
    "\n",
    "        dx = dx_pad[:, :, P_h:H + P_h, P_w:W + P_w]\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p=0.5):\n",
    "        # Dropout probability (probability of dropping a neuron)\n",
    "        self.p = p\n",
    "        # Mask to store which neurons are dropped\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        # Apply dropout only during training\n",
    "        if training:\n",
    "            # Generate binary mask with probability (1 - p)\n",
    "            # Scale the mask by (1 - p) to maintain the expected value\n",
    "            self.mask = np.random.binomial(1, 1 - self.p, size=x.shape) / (1 - self.p)\n",
    "            # Apply mask to input\n",
    "            return x * self.mask\n",
    "\n",
    "        # During inference, return the input as is\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d(Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        # Small constant for numerical stability\n",
    "        self.eps = eps\n",
    "        # Momentum for running mean and variance calculation\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable scale parameter (gamma) initialized to ones\n",
    "        self.gamma = {'val': np.ones(num_features), 'grad': np.zeros(num_features)}\n",
    "        # Learnable shift parameter (beta) initialized to zeros\n",
    "        self.beta = {'val': np.zeros(num_features), 'grad': np.zeros(num_features)}\n",
    "        # Add gamma and beta to trainable parameters\n",
    "        self.params.extend([self.gamma, self.beta])\n",
    "        \n",
    "        # Running mean and variance for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        # Cache for storing intermediate values during forward pass\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            # Calculate mean and variance across batch and spatial dimensions\n",
    "            mu = x.mean(axis=(0,2,3), keepdims=True)\n",
    "            var = x.var(axis=(0,2,3), keepdims=True)\n",
    "            # Update running mean and variance with momentum\n",
    "            self.running_mean = (1 - self.momentum)*self.running_mean + self.momentum*mu.squeeze()\n",
    "            self.running_var = (1 - self.momentum)*self.running_var + self.momentum*var.squeeze()\n",
    "        else:\n",
    "            # Use running mean and variance during inference\n",
    "            mu = self.running_mean.reshape(1,-1,1,1)\n",
    "            var = self.running_var.reshape(1,-1,1,1)\n",
    "            \n",
    "        # Normalize the input\n",
    "        x_hat = (x - mu) / np.sqrt(var + self.eps)\n",
    "        # Scale and shift the normalized values\n",
    "        out = self.gamma['val'].reshape(1,-1,1,1) * x_hat + self.beta['val'].reshape(1,-1,1,1)\n",
    "        # Cache values needed for backward pass\n",
    "        self.cache = (x, x_hat, mu, var)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Retrieve cached values from forward pass\n",
    "        x, x_hat, mu, var = self.cache\n",
    "        # Get input dimensions\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        # Compute gradients for gamma and beta\n",
    "        dgamma = (dout * x_hat).sum(axis=(0,2,3))\n",
    "        dbeta = dout.sum(axis=(0,2,3))\n",
    "        # Accumulate gradients\n",
    "        self.gamma['grad'] += dgamma\n",
    "        self.beta['grad'] += dbeta\n",
    "        \n",
    "        # Compute gradient of normalized input\n",
    "        dx_hat = dout * self.gamma['val'].reshape(1,-1,1,1)\n",
    "        # Compute gradient of variance\n",
    "        dvar = (dx_hat * (x - mu) * -0.5 * (var + self.eps)**-1.5).sum(axis=(0,2,3), keepdims=True)\n",
    "        # Compute gradient of mean\n",
    "        dmu = (dx_hat * -1.0 / np.sqrt(var + self.eps)).sum(axis=(0,2,3), keepdims=True) + dvar * -2.0 * (x - mu).mean(axis=(0,2,3), keepdims=True)\n",
    "        # Compute gradient of input\n",
    "        dx = dx_hat / np.sqrt(var + self.eps) + dvar * 2.0 * (x - mu) / (N*H*W) + dmu / (N*H*W)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_pred, y_true, gamma=2.0, alpha=0.25):\n",
    "    # Clip predictions to prevent log(0) or log(1) which would result in NaN\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    ce_loss = -y_true * np.log(y_pred) - (1.0 - y_true) * np.log(1.0 - y_pred)\n",
    "    \n",
    "    # Calculate probability of true class\n",
    "    pt = np.where(y_true == 1, y_pred, 1.0 - y_pred)\n",
    "    \n",
    "    # Calculate focal loss by modulating cross-entropy loss\n",
    "    focal_loss = alpha * np.power(1.0 - pt, gamma) * ce_loss\n",
    "    \n",
    "    # Return mean focal loss over the batch\n",
    "    return np.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SGD:\n",
    "#     def __init__(self, lr=0.01, momentum=0.0):\n",
    "#         # Learning rate for parameter updates\n",
    "#         self.lr = lr\n",
    "#         # Momentum factor for velocity calculation\n",
    "#         self.momentum = momentum\n",
    "#         # Dictionary to store velocity for each parameter\n",
    "#         self.v = {}\n",
    "\n",
    "#     def step(self, params, grads):\n",
    "#         # Update each parameter using SGD with momentum\n",
    "#         for key in params.keys():\n",
    "#             # Initialize velocity if not already done\n",
    "#             if key not in self.v:\n",
    "#                 self.v[key] = np.zeros_like(params[key])\n",
    "#             # Update velocity using momentum and current gradient\n",
    "#             self.v[key] = self.momentum * self.v[key] + self.lr * grads[key]\n",
    "#             # Update parameter using the calculated velocity\n",
    "#             params[key] -= self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        # Learning rate for parameter updates\n",
    "        self.lr = lr\n",
    "        # Exponential decay rate for the first moment estimates\n",
    "        self.beta1 = beta1\n",
    "        # Exponential decay rate for the second moment estimates\n",
    "        self.beta2 = beta2\n",
    "        # Small constant for numerical stability\n",
    "        self.eps = eps\n",
    "        # Dictionary to store first moment estimates (mean)\n",
    "        self.m = {}\n",
    "        # Dictionary to store second moment estimates (uncentered variance)\n",
    "        self.v = {}\n",
    "        # Time step counter\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        # Update each parameter using Adam optimization\n",
    "        for key in params.keys():\n",
    "            # Initialize moment estimates if not already done\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "            # Update first moment estimate (mean)\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            # Update second moment estimate (uncentered variance)\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * grads[key] ** 2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            # Update parameter using Adam update rule\n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaptiveAvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAvgPool2d(Module):\n",
    "    def __init__(self, output_size=(1,1)):\n",
    "        super().__init__()\n",
    "        # Target output size (height, width)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get input dimensions: batch size, channels, height, width\n",
    "        N, C, H, W = x.shape\n",
    "        # Target output height and width\n",
    "        H_out, W_out = self.output_size\n",
    "        # Calculate stride sizes based on input and output dimensions\n",
    "        h_stride = H // H_out\n",
    "        w_stride = W // W_out\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        out = np.zeros((N, C, H_out, W_out))\n",
    "        # Perform adaptive average pooling\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                # Calculate window boundaries\n",
    "                h_start = i * h_stride\n",
    "                h_end = min(h_start + h_stride, H)\n",
    "                w_start = j * w_stride\n",
    "                w_end = min(w_start + w_stride, W)\n",
    "                # Compute mean of the window\n",
    "                out[:,:,i,j] = x[:,:,h_start:h_end,w_start:w_end].mean(axis=(2,3))\n",
    "        # Cache input shape for backward pass\n",
    "        self.cache = x.shape\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Get output gradient dimensions\n",
    "        N, C, H_out, W_out = dout.shape\n",
    "        # Retrieve cached input shape\n",
    "        orig_shape = self.cache\n",
    "        H, W = orig_shape[2], orig_shape[3]\n",
    "        # Calculate stride sizes\n",
    "        h_stride = H // H_out\n",
    "        w_stride = W // W_out\n",
    "        \n",
    "        # Initialize input gradient tensor\n",
    "        dx = np.zeros(orig_shape)\n",
    "        # Distribute gradients back to input\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                # Calculate window boundaries\n",
    "                h_start = i * h_stride\n",
    "                h_end = min(h_start + h_stride, H)\n",
    "                w_start = j * w_stride\n",
    "                w_end = min(w_start + w_stride, W)\n",
    "                # Distribute gradient equally across the window\n",
    "                dx[:,:,h_start:h_end,w_start:w_end] += dout[:,:,i:i+1,j:j+1] / (h_stride * w_stride)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        # Whether to downsample the input\n",
    "        self.downsample = downsample\n",
    "        # Stride for convolution operations\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        # First convolution layer\n",
    "        self.conv1 = Conv2d(in_channels, out_channels, 3, stride, bias=False)\n",
    "        # First batch normalization layer\n",
    "        self.bn1 = BatchNorm2d(out_channels)\n",
    "        # First ReLU activation\n",
    "        self.relu1 = ReLU()\n",
    "        # Second convolution layer\n",
    "        self.conv2 = Conv2d(out_channels, out_channels, 3, bias=False)\n",
    "        # Second batch normalization layer\n",
    "        self.bn2 = BatchNorm2d(out_channels)\n",
    "        # Second ReLU activation\n",
    "        self.relu2 = ReLU()\n",
    "\n",
    "        # Shortcut connection\n",
    "        if downsample or in_channels != out_channels:\n",
    "            # If downsampling or channel mismatch, use a 1x1 convolution\n",
    "            self.shortcut = Sequential(\n",
    "                Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            # Otherwise, use identity mapping\n",
    "            self.shortcut = lambda x: x\n",
    "\n",
    "        # Collect all parameters\n",
    "        self.params.extend(self.conv1.params + self.bn1.params + self.conv2.params + self.bn2.params)\n",
    "        if hasattr(self.shortcut, 'params'):\n",
    "            self.params.extend(self.shortcut.params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply shortcut connection\n",
    "        if hasattr(self.shortcut, 'forward'):\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "        else:\n",
    "            shortcut = self.shortcut(x)\n",
    "\n",
    "        # Main path\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        # Add shortcut to main path\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Backward pass through second ReLU\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout_main = dout\n",
    "        dout_shortcut = dout\n",
    "\n",
    "        # Backward pass through main path\n",
    "        dout = self.bn2.backward(dout_main)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout_conv1 = self.conv1.backward(dout)\n",
    "\n",
    "        # Backward pass through shortcut\n",
    "        if hasattr(self.shortcut, 'backward'):\n",
    "            dout_short = self.shortcut.backward(dout_shortcut)\n",
    "        else:\n",
    "            dout_short = dout_shortcut\n",
    "\n",
    "        # Sum gradients from main path and shortcut\n",
    "        return dout_conv1 + dout_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualMNIST network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMNIST(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Number of input channels for the first layer\n",
    "        self.in_channels = 64\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = Conv2d(1, 64, 3, bias=False)\n",
    "        # Batch normalization layer\n",
    "        self.bn1 = BatchNorm2d(64)\n",
    "        # ReLU activation\n",
    "        self.relu = ReLU()\n",
    "        # Three residual layers with increasing channels\n",
    "        self.layer1 = self._make_layer(64, 2)\n",
    "        self.layer2 = self._make_layer(128, 2, downsample=True)\n",
    "        self.layer3 = self._make_layer(256, 2, downsample=True)\n",
    "        # Adaptive average pooling to reduce spatial dimensions to 1x1\n",
    "        self.avgpool = AdaptiveAvgPool2d((1,1))\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = Linear(256, 10)\n",
    "        \n",
    "        # Collect all parameters from layers\n",
    "        self.params = []\n",
    "        for layer in [self.conv1, self.bn1, self.layer1, self.layer2, self.layer3, self.avgpool, self.fc]:\n",
    "            self.params += layer.params\n",
    "    \n",
    "    def _make_layer(self, out_channels, num_blocks, downsample=False):\n",
    "        # Create a layer with multiple residual blocks\n",
    "        layers = []\n",
    "        # First block may downsample\n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        # Add remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.bn1.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.layer1.forward(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        x = self.layer3.forward(x)\n",
    "        x = self.avgpool.forward(x)\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Backward pass through the network\n",
    "        dout = self.fc.backward(dout)\n",
    "        # Reshape gradient for adaptive pooling\n",
    "        dout = dout.reshape(dout.shape[0], 256, 1, 1)\n",
    "        dout = self.avgpool.backward(dout)\n",
    "        dout = self.layer3.backward(dout)\n",
    "        dout = self.layer2.backward(dout)\n",
    "        dout = self.layer1.backward(dout)\n",
    "        dout = self.relu.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Sequential(\n",
    "#     LinearNoBias(784, 512), Softplus(),\n",
    "#     LinearNoBias(512, 256), Softplus(),\n",
    "#     LinearNoBias(256, 128), Softplus(),\n",
    "#     LinearNoBias(128, 10),\n",
    "# )\n",
    "\n",
    "net = ResNetMNIST()\n",
    "\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch and torchvision provide some very handy utilities for dataset loading\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "num_workers = 6\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.MNIST(\n",
    "        root=\"./data\", train=is_train, download=True, \n",
    "        transform=tv_transforms.Compose([ # preprocessing pipeline for input images\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]))\n",
    "    loader[data_type] = DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch=  1, iter=  200] loss: 0.472\n",
      "[epoch=  1, iter=  400] loss: 0.074\n",
      "[epoch=  2, iter=  200] loss: 0.045\n",
      "[epoch=  2, iter=  400] loss: 0.036\n",
      "[epoch=  3, iter=  200] loss: 0.024\n",
      "[epoch=  3, iter=  400] loss: 0.026\n",
      "[epoch=  4, iter=  200] loss: 0.016\n",
      "[epoch=  4, iter=  400] loss: 0.017\n",
      "[epoch=  5, iter=  200] loss: 0.009\n",
      "[epoch=  5, iter=  400] loss: 0.010\n",
      "[epoch=  6, iter=  200] loss: 0.006\n",
      "[epoch=  6, iter=  400] loss: 0.007\n",
      "[epoch=  7, iter=  200] loss: 0.004\n",
      "[epoch=  7, iter=  400] loss: 0.004\n",
      "[epoch=  8, iter=  200] loss: 0.002\n",
      "[epoch=  8, iter=  400] loss: 0.003\n",
      "[epoch=  9, iter=  200] loss: 0.002\n",
      "[epoch=  9, iter=  400] loss: 0.002\n",
      "[epoch= 10, iter=  200] loss: 0.001\n",
      "[epoch= 10, iter=  400] loss: 0.001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(net.params, lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.numpy(), target.numpy()\n",
    "        # img = img.reshape(-1, 784)\n",
    "        \n",
    "        loss = loss_fn.forward(net.forward(img), target)\n",
    "\n",
    "        net.backward(loss_fn.backward(1.0))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.37%\n"
     ]
    }
   ],
   "source": [
    "# for each test image\n",
    "correct, total = 0, 0\n",
    "for img, target in loader[\"test\"]:\n",
    "    img, target = img.numpy(), target.numpy()\n",
    "    # img = img.reshape(-1, 784)\n",
    "\n",
    "    # make prediction\n",
    "    pred = net.forward(img)\n",
    "\n",
    "    # accumulate\n",
    "    total += len(target)\n",
    "    correct += (np.argmax(pred, axis=1) == target).sum()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
